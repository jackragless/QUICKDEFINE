{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrladidadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mrladidadi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/mrladidadi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import nltk\n",
    "import numpy\n",
    "import random\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "new_stopwords = set(stopwords.words('english')) - {'not'}\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mined_sites_0_30.txt\", \"rb\") as fp:   # Unpickling\n",
    "    all_sites = pickle.load(fp)\n",
    "with open(\"mined_sites_filtered_0_30.txt\", \"rb\") as fp2:   # Unpickling\n",
    "    filt_sites = pickle.load(fp2)\n",
    "with open(\"syntactic_analysis/constituency_parse_dict_top.txt\", \"rb\") as fp3:   # Unpickling\n",
    "    const_parse_lvl1 = pickle.load(fp3)[0]\n",
    "const_parse_lvl2 = list(pd.read_csv(\"2_lvl_const_parse.csv\")['cons_parse'])\n",
    "with open(\"kw_clean.txt\", \"rb\") as fp4:   # Unpickling\n",
    "    keyword = pickle.load(fp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_foreign(sent):\n",
    "    condition = True\n",
    "    for x in str(sent).split():    \n",
    "        if not x.isalnum():\n",
    "            condition = False\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidate(sent, kw):\n",
    "    if clean(kw, no_punct=True, lang=\"en\") in sent[0:30+len(kw)]:\n",
    "#     if clean(kw, no_punct=True, lang=\"en\") in sent:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(text):\n",
    "    clean = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    clean = re.sub(r'\\([^)]*\\)', '', clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lematize(text,lemat_bool):\n",
    "    if lemat_bool == True:\n",
    "        clean_sent = clean_sent.split()\n",
    "        wl = WordNetLemmatizer()\n",
    "        clean_sent = [wl.lemmatize(word) for word in clean_sent if not word in new_stopwords]\n",
    "        clean_sent = ' '.join(clean_sent)\n",
    "        return clean_sent\n",
    "    \n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_driver(df, lemat_bool):\n",
    "    corpus = []\n",
    "    for site in df:\n",
    "        temp_arr = []\n",
    "        \n",
    "        text = regex(site['text'])\n",
    "        sent = nltk.sent_tokenize(text) \n",
    "        \n",
    "        for j in range(len(sent)):\n",
    "            \n",
    "            clean_sent = clean(sent[j], no_punct=True, lang=\"en\")\n",
    "            clean_sent = lematize(clean_sent, lemat_bool)\n",
    "              \n",
    "            if no_foreign(clean_sent) and candidate(clean_sent, site['kw']) and len(clean_sent.split())<=60:\n",
    "                temp_arr.append(clean_sent)\n",
    "        \n",
    "        corpus.append({'kwn':site['kwn'], 'kw':site['kw'], 'link':site['link'], 'sent':temp_arr} )\n",
    "          \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_clean = clean_driver(all_sites, False)\n",
    "filt_sites_clean = clean_driver(filt_sites, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_filter(master, kwn):\n",
    "    final = []\n",
    "\n",
    "    for i in master:\n",
    "        if i['kwn'] == kwn:\n",
    "            for j in i['sent']:\n",
    "                final.append(j)\n",
    "\n",
    "    if len(final)>=10:\n",
    "        randarr = []\n",
    "        for i in range(10):\n",
    "            randarr.append(final[random.randint(0,len(final)-1)])\n",
    "        return randarr\n",
    "    else:\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_lvl1(main):\n",
    "    main = main['hierplane_tree']['root']['children']\n",
    "    final = ''\n",
    "    for i in range(len(main)):\n",
    "        final += main[i]['nodeType'] + ' '\n",
    "    return final.strip()\n",
    "\n",
    "def tree_lvl2(main):\n",
    "    main = main['hierplane_tree']['root']['children']\n",
    "    final = ''\n",
    "    for i in range(len(main)):\n",
    "        if 'children' in main[i]:\n",
    "            for j in range(len(main[i]['children'])):\n",
    "                final += main[i]['children'][j]['nodeType'] + ' '\n",
    "        else:\n",
    "            final += '?' + ' '\n",
    "    return final.strip()\n",
    "\n",
    "\n",
    "def const_parse_filter(string, lvl_num):\n",
    "    temp_const = predictor.predict(sentence = string)\n",
    "    if lvl_num == 1:\n",
    "        if tree_lvl1(temp_const) in const_parse_lvl1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    if lvl_num == 2:\n",
    "        if tree_lvl1(temp_const) + ' /// ' + tree_lvl2(temp_const) in const_parse_lvl2:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def const_parse_driver(master, kwn, lvl_num):\n",
    "    final = []\n",
    "    count = 0\n",
    "    for i in master:\n",
    "        print(int(count/16)*100)\n",
    "        if i['kwn'] == kwn:\n",
    "            for j in i['sent']:\n",
    "                if const_parse_filter(j, lvl_num):\n",
    "                    final.append(j)\n",
    "        count += 1\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filt_sites_clean:\n",
    "    i['pos'] = []\n",
    "    for j in i['sent']:\n",
    "        temp_const = predictor.predict(sentence = j)\n",
    "        i['pos'].append(tree_lvl1(temp_const) + ' /// ' + tree_lvl2(temp_const))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kwn': 25,\n",
       " 'kw': 'regular expression',\n",
       " 'link': 'https://en.wikipedia.org/wiki/Regular_expression',\n",
       " 'sent': ['a regular expression is a sequence of characters that define a search pattern',\n",
       "  'regular expressions are used in search engines search and replace dialogs of word processors and text editors in text processing utilities such as sed and awk and in lexical analysis',\n",
       "  'each character in a regular expression is either a metacharacter having a special meaning or a regular character that has a literal meaning',\n",
       "  'a very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor the regular expression serialie matches both serialise and serialize',\n",
       "  'regular expressions entered popular use from 1968 in two uses pattern matching in a text editor and lexical analysis in a compiler',\n",
       "  'a regular expression often called a pattern specifies a set of strings required for a particular purpose',\n",
       "  'the precise syntax for regular expressions varies among tools and with context more detail is given in syntax',\n",
       "  'regular expressions describe regular languages in formal language theory',\n",
       "  'regular expressions consist of constants which denote sets of strings and operator symbols which denote operations over these sets',\n",
       "  'however it can make a regular expression much more conciseeliminating all complement operators from a regular expression can cause a double exponential blowup of its length',\n",
       "  'regular expressions in this sense can express the regular languages exactly the class of languages accepted by deterministic finite automata',\n",
       "  'however a regular expression to answer the same problem of divisibility by 11 is at least multiple megabytes in length',\n",
       "  'given a regular expression thompsons construction algorithm computes an equivalent nondeterministic finite automaton',\n",
       "  'as simple as the regular expressions are there is no method to systematically rewrite them to some normal form',\n",
       "  'constructing the dfa for a regular expression of size m has the time and memory cost of o but it can be run on a string of size n in time o',\n",
       "  'standard posix regular expressions are different',\n",
       "  'a regular expressions matcher',\n",
       "  'perlre perl regular expressions',\n",
       "  'a formal study of practical regular expressions',\n",
       "  'regular expressions with backreferences polynomialtime matching techniques',\n",
       "  'regular expressions end of string',\n",
       "  'patterns automata and regular expressions',\n",
       "  'regular expressions',\n",
       "  'regular expression matching can be simple and fast',\n",
       "  'sams teach yourself regular expressions in 10 minutes',\n",
       "  'mastering regular expressions',\n",
       "  'regular expressions cookbook',\n",
       "  'real world regular expressions with java 14',\n",
       "  'regular expression pocket reference',\n",
       "  'programming techniques regular expression search algorithm'],\n",
       " 'pos': ['NP VP /// DT JJ NN VBZ NP',\n",
       "  'NP VP /// JJ NNS VBP VP',\n",
       "  'NP VP /// NP PP VBZ NP',\n",
       "  'NP VP /// NP PP VBZ S',\n",
       "  'NP VP /// JJ NNS VBD NP PP IN NP VBG IN NP PP',\n",
       "  'NP VP /// NP VP VBZ NP',\n",
       "  'NP VP CC PP NP VBZ VP /// NP PP VBZ PP ? IN NP JJR NN ? VBN PP',\n",
       "  'NP VP /// JJ NNS VBP NP PP',\n",
       "  'NP VP /// JJ NNS VBP IN NP',\n",
       "  'RB NP VP VP /// ? PRP MD VP MD VP',\n",
       "  'NP VP /// NP PP MD VP',\n",
       "  'ADVP NP VP /// RB NP SBAR VBZ NP',\n",
       "  'VBN NP VP /// ? DT JJ NN NNS NN NN VBZ NP',\n",
       "  'ADJP IN S NP VP /// RB JJ ? NP VP EX VBZ NP',\n",
       "  'S VP CC S /// VP VBZ NP ? NP VP',\n",
       "  'NP VP /// NP JJ NNS VBP ADJP',\n",
       "  'NP VP /// DT JJ NNS VBP',\n",
       "  'NNP NNP JJ NNS /// ? ? ? ?',\n",
       "  'NP PP /// DT JJ NN IN NP',\n",
       "  'NP PP /// JJ NNS IN NP',\n",
       "  'NP NP /// JJ NNS NN PP',\n",
       "  'NNS NP CC NP /// ? NNP ? JJ NNS',\n",
       "  'JJ NNS /// ? ?',\n",
       "  'NP VP /// JJ NN NN MD VP',\n",
       "  'NP VP /// NNS VBP NP NP PP',\n",
       "  'VP /// VBG NP',\n",
       "  'JJ NNS NN /// ? ? ?',\n",
       "  'NP PP /// NP JJ NNS IN NP',\n",
       "  'JJ NN NN NN /// ? ? ? ?',\n",
       "  'NP NP /// NN NNS JJ NN NN NN']}"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_sites_clean[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'base64 is an encoding scheme used to represent binary data in an ascii format',\n",
       " 'encodetostring returns the base64 encoding of src',\n",
       " 'newdecoder constructs a new base64 stream decoder',\n",
       " 'newencoder returns a new base64 stream encoder',\n",
       " 'stdencoding is the standard base64 encoding as defined in rfc 4648',\n",
       " 'urlencoding is the alternate base64 encoding defined in rfc 4648'}"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent = []\n",
    "all_pos = []\n",
    "for i in filt_sites_clean:\n",
    "    if i['kwn'] == 17:\n",
    "        all_sent += i['sent']\n",
    "        all_pos += i['pos']\n",
    "\n",
    "FINAL = []\n",
    "\n",
    "for j in range(len(all_pos)):\n",
    "    if all_pos[j] in const_parse_lvl2:\n",
    "        FINAL.append(all_sent[j])\n",
    "        \n",
    "print(len(set(FINAL)))\n",
    "set(FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_sites_clean:\n",
    "    i['pos2'] = []\n",
    "    for j in i['pos']:\n",
    "        temp = ''\n",
    "        for k in j.split():\n",
    "            if(k=='///'):\n",
    "                break\n",
    "            temp += k + ' '\n",
    "        i['pos2'].append(temp.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'donna mcmaster maintenance mode gives you the option of letting your visitors know that you are working on your website updating or fixing problems',\n",
       " 'maintenance mode suspends the following featuresrules and monitors rules and monitorsnotifications notificationsautomatic responses automatic responsesstate changes state changesnew alerts new alertsfor example an exchange mailbox role running on a windows server will have an exchange server service pack applied',\n",
       " 'our maintenance mode is flexible customizable and controlled remotely from your managewp dashboard',\n",
       " 'thats what maintenance mode does and its why wordpress has it as a builtin feature whenever you update wordpress itself wordpress will put your website into maintenance mode automaticallybut the problem with wordpress builtin maintenance mode is that you cant pull it up whenever you need it',\n",
       " 'the maintenance mode page appears',\n",
       " 'this cli maintenance mode is often used during cli version upgrades',\n",
       " 'users will only see the maintenance mode message when they attempt to access a course'}"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sent = []\n",
    "all_pos = []\n",
    "for i in all_sites_clean:\n",
    "    if i['kwn'] == 18:\n",
    "        all_sent += i['sent']\n",
    "        all_pos += i['pos2']\n",
    "\n",
    "FINAL = []\n",
    "\n",
    "for j in range(len(all_pos)):\n",
    "    if all_pos[j] in const_parse_lvl1:\n",
    "        FINAL.append(all_sent[j])\n",
    "        \n",
    "print(len(set(FINAL)))\n",
    "set(FINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must be given entire array to function\n",
    "def textrank(para, ratio):\n",
    "    if len(para) >=2:\n",
    "        temp_para = ''\n",
    "        for i in para:\n",
    "            temp_para += i + '. '\n",
    "        return summarize(temp_para, ratio=ratio, word_count=None, split=True)\n",
    "    else:\n",
    "        return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_driver(master, kwn, ratio):\n",
    "    final = []\n",
    "    count = 0\n",
    "    for i in master:\n",
    "        if i['kwn'] == kwn:\n",
    "            temp = textrank(i['sent'], ratio)\n",
    "            if len(temp)>=1:\n",
    "                final += temp\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
