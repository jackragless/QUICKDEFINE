{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrladidadi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mrladidadi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/mrladidadi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#required libraries\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from cleantext import clean\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import nltk\n",
    "import numpy\n",
    "import random\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "new_stopwords = set(stopwords.words('english')) - {'not'}\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.structured_prediction\n",
    "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/elmo-constituency-parser-2020.02.10.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing site objects, const_parse_data etc\n",
    "with open(\"mined_sites_0_30.txt\", \"rb\") as fp:   # Unpickling\n",
    "    all_sites = pickle.load(fp)\n",
    "with open(\"mined_sites_filtered_0_30.txt\", \"rb\") as fp2:   # Unpickling\n",
    "    filt_sites = pickle.load(fp2)\n",
    "with open(\"syntactic_analysis/constituency_parse_dict_top.txt\", \"rb\") as fp3:   # Unpickling\n",
    "    const_parse_lvl1 = pickle.load(fp3)[0]\n",
    "const_parse_lvl2 = list(pd.read_csv(\"2_lvl_const_parse.csv\")['cons_parse'])\n",
    "with open(\"kw_clean.txt\", \"rb\") as fp4:   # Unpickling\n",
    "    keyword = pickle.load(fp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns true if string does not contain any foreign chars\n",
    "def no_foreign(sent):\n",
    "    condition = True\n",
    "    for x in str(sent).split():    \n",
    "        if not x.isalnum():\n",
    "            condition = False\n",
    "    return condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns true if string contains keyword towards start of sentence\n",
    "def candidate(sent, kw):\n",
    "    if clean(kw, no_punct=True, lang=\"en\") in sent[0:30+len(kw)]:\n",
    "#     if clean(kw, no_punct=True, lang=\"en\") in sent:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes all content within/including [] and () brackets\n",
    "def regex(text):\n",
    "    clean = re.sub(r'\\[[^\\]]*\\]', '', text)\n",
    "    clean = re.sub(r'\\([^)]*\\)', '', clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lematises sentence / removes stopwords\n",
    "def lematize(text,lemat_bool): #only returns lemmatised string if bool == True\n",
    "    if lemat_bool == True:\n",
    "        clean_sent = clean_sent.split()\n",
    "        wl = WordNetLemmatizer()\n",
    "        clean_sent = [wl.lemmatize(word) for word in clean_sent if not word in new_stopwords]\n",
    "        clean_sent = ' '.join(clean_sent)\n",
    "        return clean_sent\n",
    "    \n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all of above functions to clean an array of site objects\n",
    "def clean_driver(df, lemat_bool):\n",
    "    corpus = []\n",
    "    for site in df:\n",
    "        temp_arr = []\n",
    "        \n",
    "        text = regex(site['text'])\n",
    "        sent = nltk.sent_tokenize(text) \n",
    "        \n",
    "        for j in range(len(sent)):\n",
    "            \n",
    "            clean_sent = clean(sent[j], no_punct=True, lang=\"en\")\n",
    "            clean_sent = lematize(clean_sent, lemat_bool)\n",
    "              \n",
    "            if no_foreign(clean_sent) and candidate(clean_sent, site['kw']) and len(clean_sent.split())<=60:\n",
    "                temp_arr.append(clean_sent)\n",
    "        \n",
    "        corpus.append({'kwn':site['kwn'], 'kw':site['kw'], 'link':site['link'], 'sent':temp_arr} )\n",
    "          \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_clean = clean_driver(all_sites, False)\n",
    "filt_sites_clean = clean_driver(filt_sites, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random definiton model (used as benchmark to compare to other models)\n",
    "def random_filter(master, kwn):\n",
    "    final = []\n",
    "\n",
    "    for i in master:\n",
    "        if i['kwn'] == kwn:\n",
    "            for j in i['sent']:\n",
    "                final.append(j)\n",
    "\n",
    "    if len(final)>=10:\n",
    "        randarr = []\n",
    "        for i in range(10):\n",
    "            randarr.append(final[random.randint(0,len(final)-1)])\n",
    "        return randarr\n",
    "    else:\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplifies AllenNLP constituency parse object into 'lvl1 /// lvl2'\n",
    "def tree_lvl1(main):\n",
    "    main = main['hierplane_tree']['root']['children']\n",
    "    final = ''\n",
    "    for i in range(len(main)):\n",
    "        final += main[i]['nodeType'] + ' '\n",
    "    return final.strip()\n",
    "\n",
    "def tree_lvl2(main):\n",
    "    main = main['hierplane_tree']['root']['children']\n",
    "    final = ''\n",
    "    for i in range(len(main)):\n",
    "        if 'children' in main[i]:\n",
    "            for j in range(len(main[i]['children'])):\n",
    "                final += main[i]['children'][j]['nodeType'] + ' '\n",
    "        else:\n",
    "            final += '?' + ' '\n",
    "    return final.strip()\n",
    "\n",
    "\n",
    "def const_parse_filter(string, lvl_num):\n",
    "    temp_const = predictor.predict(sentence = string)\n",
    "    if lvl_num == 1:\n",
    "        if tree_lvl1(temp_const) in const_parse_lvl1:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    if lvl_num == 2:\n",
    "        if tree_lvl1(temp_const) + ' /// ' + tree_lvl2(temp_const) in const_parse_lvl2:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drives const_parse functions above\n",
    "def const_parse_driver(master, kwn, lvl_num):\n",
    "    final = []\n",
    "    count = 0\n",
    "    for i in master:\n",
    "        print(int(count/16)*100)\n",
    "        if i['kwn'] == kwn:\n",
    "            for j in i['sent']:\n",
    "                if const_parse_filter(j, lvl_num):\n",
    "                    final.append(j)\n",
    "        count += 1\n",
    "    return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns nost \"important\" string from an array (Gensim)\n",
    "def textrank(para, ratio):\n",
    "    if len(para) >=2:\n",
    "        temp_para = ''\n",
    "        for i in para:\n",
    "            temp_para += i + '. '\n",
    "        return summarize(temp_para, ratio=ratio, word_count=None, split=True)\n",
    "    else:\n",
    "        return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
